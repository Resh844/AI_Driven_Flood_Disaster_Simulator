{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c003e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Flood Disaster Simulation Backend (FINAL + INPUT VALIDATION)\n",
    "# Flask + ngrok | Frontend-Compatible | PPT-Compliant\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q flask flask-cors pillow requests numpy opencv-python-headless torch torchvision diffusers transformers accelerate pyngrok scikit-image perlin-noise timm > /dev/null || true\n",
    "\n",
    "# -------------------------\n",
    "# 1) Imports\n",
    "# -------------------------\n",
    "import os, io, json, time, math, traceback\n",
    "from flask import Flask, request, send_file, make_response, jsonify\n",
    "from flask_cors import CORS\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "from pyngrok import ngrok\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.linalg import sqrtm\n",
    "from perlin_noise import PerlinNoise\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    AutoImageProcessor, AutoModelForSemanticSegmentation,\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "# ============================================================\n",
    "# 2) CONFIGURATION \n",
    "# ============================================================\n",
    "\n",
    "HF_TOKEN = \"YOUR HUGGING FACE TOKEN\"\n",
    "NGROK_TOKEN = \"YOUR NGROK_TOKEN\"\n",
    "\n",
    "# ---- Google Earth Engine (Conceptual) ----\n",
    "GEE_PROJECT_ID = \"GOOGLE PROJECT_ID\"\n",
    "GEE_DATASET = \"COPERNICUS/S2\"\n",
    "\n",
    "# ---- ChatGPT / LLM (Conceptual) ----\n",
    "CHATGPT_MODEL_NAME = \"GPT-4 / GPT-3.5 (conceptual)\"\n",
    "\n",
    "# ============================================================\n",
    "# 3) App Setup\n",
    "# ============================================================\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Load Active Models\n",
    "# ============================================================\n",
    "\n",
    "# ---- CLIP (USED for validation) ----\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "# ---- SegFormer ----\n",
    "seg_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n",
    "seg_model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b4-finetuned-ade-512-512\"\n",
    ").to(device)\n",
    "seg_model.eval()\n",
    "\n",
    "# ============================================================\n",
    "# 5)  MODELS \n",
    "# ============================================================\n",
    "\n",
    "# ---- BLIP ----\n",
    "try:\n",
    "    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    ).to(device)\n",
    "    blip_model.eval()\n",
    "except Exception:\n",
    "    blip_model = None\n",
    "    blip_processor = None\n",
    "\n",
    "# ---- GPT (local placeholder) ----\n",
    "try:\n",
    "    gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "    gpt_model.eval()\n",
    "except Exception:\n",
    "    gpt_model = None\n",
    "    gpt_tokenizer = None\n",
    "\n",
    "# ---- Stable Diffusion (optional) ----\n",
    "sd_pipe = None\n",
    "try:\n",
    "    sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-inpainting\",\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        use_auth_token=HF_TOKEN\n",
    "    ).to(device)\n",
    "    sd_pipe.safety_checker = None\n",
    "except Exception:\n",
    "    sd_pipe = None\n",
    "\n",
    "# ============================================================\n",
    "# 6) PLACEHOLDER FUNCTIONS \n",
    "# ============================================================\n",
    "\n",
    "def fetch_image_from_gee(lat, lon):\n",
    "    \"\"\"\n",
    "    Placeholder for Google Earth Engine Sentinel-2 imagery.\n",
    "    Not invoked due to authentication constraints.\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "def generate_prompt_with_blip_and_gpt(pil_img):\n",
    "    \"\"\"\n",
    "    BLIP + GPT conceptual pipeline (not used).\n",
    "    \"\"\"\n",
    "    return {\"caption\": \"urban area\", \"prompt\": \"urban area affected by flooding\"}\n",
    "\n",
    "def enhance_prompt_with_chatgpt(prompt):\n",
    "    \"\"\"\n",
    "    ChatGPT conceptual enhancement.\n",
    "    \"\"\"\n",
    "    return prompt + \" with post-disaster flood damage\"\n",
    "\n",
    "# ============================================================\n",
    "# 7) TILE FETCHING\n",
    "# ============================================================\n",
    "\n",
    "def latlon_to_tile(lat, lon, zoom):\n",
    "    lat_rad = math.radians(lat)\n",
    "    n = 2.0 ** zoom\n",
    "    return int((lon + 180) / 360 * n), int(\n",
    "        (1 - math.log(math.tan(lat_rad) + 1 / math.cos(lat_rad)) / math.pi) / 2 * n\n",
    "    )\n",
    "\n",
    "def fetch_512_tile(lat, lon, zoom=16):\n",
    "    cx, cy = latlon_to_tile(lat, lon, zoom)\n",
    "    tiles = []\n",
    "    for dy in [0,1]:\n",
    "        for dx in [0,1]:\n",
    "            r = requests.get(\n",
    "                f\"https://mt1.google.com/vt/lyrs=s&x={cx+dx}&y={cy+dy}&z={zoom}\",\n",
    "                timeout=20\n",
    "            )\n",
    "            tiles.append(Image.open(io.BytesIO(r.content)).convert(\"RGB\"))\n",
    "    img = Image.new(\"RGB\", (512,512))\n",
    "    img.paste(tiles[0], (0,0)); img.paste(tiles[1], (256,0))\n",
    "    img.paste(tiles[2], (0,256)); img.paste(tiles[3], (256,256))\n",
    "    return img\n",
    "\n",
    "# ============================================================\n",
    "# 8) VALIDATION + CORE PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def is_satellite_image(pil_img):\n",
    "    \"\"\"\n",
    "    CLIP-based validation to reject human / street-level images.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        \"a satellite image\",\n",
    "        \"an aerial view\",\n",
    "        \"a top-down satellite photograph\"\n",
    "    ]\n",
    "    negative = [\n",
    "        \"a portrait photo\",\n",
    "        \"a human face\",\n",
    "        \"a street-level photo\",\n",
    "        \"a selfie\"\n",
    "    ]\n",
    "    inputs = clip_processor(\n",
    "        text=prompts + negative,\n",
    "        images=pil_img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = clip_model(**inputs).logits_per_image.softmax(dim=1)[0]\n",
    "\n",
    "    pos_score = logits[:len(prompts)].mean().item()\n",
    "    neg_score = logits[len(prompts):].mean().item()\n",
    "    return pos_score > neg_score + 0.03\n",
    "\n",
    "def compute_water_mask_rgb(rgb_pil):\n",
    "    arr = np.array(rgb_pil).astype(np.float32)\n",
    "    ndwi = (arr[:,:,1] - arr[:,:,2]) / (arr[:,:,1] + arr[:,:,2] + 1e-8)\n",
    "    return (ndwi > 0.03).astype(np.uint8) * 255\n",
    "\n",
    "def generate_flood_using_noise(pil_img):\n",
    "    mask = compute_water_mask_rgb(pil_img)\n",
    "    arr = np.array(pil_img)\n",
    "    arr[mask > 0] = (arr[mask > 0]*0.55 + np.array([80,120,200])*0.45).astype(np.uint8)\n",
    "    return Image.fromarray(arr), mask\n",
    "\n",
    "# ============================================================\n",
    "# 9) HOTSPOT DETECTION\n",
    "# ============================================================\n",
    "\n",
    "def detect_hotspots(mask):\n",
    "    contours,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    hotspots=[]\n",
    "    for c in contours:\n",
    "        area = int(cv2.contourArea(c))\n",
    "        if area < 60:\n",
    "            continue\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        cx = int(M[\"m10\"]/M[\"m00\"])\n",
    "        cy = int(M[\"m01\"]/M[\"m00\"])\n",
    "        sev = \"high\" if area > 4000 else \"medium\" if area > 1500 else \"low\"\n",
    "        hotspots.append({\n",
    "            \"x\": cx,\n",
    "            \"y\": cy,\n",
    "            \"area\": area,\n",
    "            \"severity\": sev\n",
    "        })\n",
    "    return hotspots\n",
    "\n",
    "# ============================================================\n",
    "# 10) METRICS\n",
    "# ============================================================\n",
    "\n",
    "_inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "_inception.fc = torch.nn.Identity()\n",
    "_inception.eval()\n",
    "\n",
    "def compute_fid(a,b):\n",
    "    tf = transforms.Compose([transforms.Resize((299,299)), transforms.ToTensor()])\n",
    "    fa = _inception(tf(a).unsqueeze(0)).detach().numpy()\n",
    "    fb = _inception(tf(b).unsqueeze(0)).detach().numpy()\n",
    "    return float(np.sum((fa.mean(0)-fb.mean(0))**2))\n",
    "\n",
    "# ============================================================\n",
    "# 11) ROUTES (Frontend-Compatible)\n",
    "# ============================================================\n",
    "\n",
    "@app.route(\"/fetch_before\")\n",
    "def fetch_before():\n",
    "    lat = float(request.args[\"lat\"])\n",
    "    lon = float(request.args[\"lon\"])\n",
    "    img = fetch_512_tile(lat, lon)\n",
    "    buf = io.BytesIO(); img.save(buf,\"PNG\"); buf.seek(0)\n",
    "    return send_file(buf, mimetype=\"image/png\")\n",
    "\n",
    "@app.route(\"/simulate\", methods=[\"POST\"])\n",
    "def simulate():\n",
    "    try:\n",
    "        if \"file\" in request.files:\n",
    "            before = Image.open(request.files[\"file\"]).convert(\"RGB\").resize((512,512))\n",
    "\n",
    "            # ðŸš« INPUT VALIDATION\n",
    "            if not is_satellite_image(before):\n",
    "                return jsonify({\n",
    "                    \"error\": \"Invalid input image. Please upload a satellite or aerial image.\"\n",
    "                }), 400\n",
    "        else:\n",
    "            before = fetch_512_tile(\n",
    "                float(request.form[\"lat\"]),\n",
    "                float(request.form[\"lon\"])\n",
    "            )\n",
    "\n",
    "        after, mask = generate_flood_using_noise(before)\n",
    "        hotspots = detect_hotspots(mask)\n",
    "\n",
    "        ssim_v = ssim(\n",
    "            np.array(before.convert(\"L\")),\n",
    "            np.array(after.convert(\"L\"))\n",
    "        )\n",
    "        fid_v = compute_fid(before, after)\n",
    "        flood_pct = round(mask.mean()/255*100,2)\n",
    "\n",
    "        metrics = {\n",
    "            \"ssim\": round(ssim_v,3),\n",
    "            \"fid\": round(fid_v,3),\n",
    "            \"flood_percent\": flood_pct\n",
    "        }\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        after.save(buf,\"PNG\")\n",
    "        buf.seek(0)\n",
    "\n",
    "        resp = make_response(send_file(buf, mimetype=\"image/png\"))\n",
    "        resp.headers[\"X-Metrics\"] = json.dumps(metrics)\n",
    "        resp.headers[\"X-Hotspots\"] = json.dumps(hotspots)\n",
    "        resp.headers[\"X-Workflow-Type\"] = \"SEG_NOISE\"\n",
    "        return resp\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/compare\", methods=[\"POST\"])\n",
    "def compare():\n",
    "    try:\n",
    "        gen = Image.open(request.files[\"generated\"]).convert(\"RGB\").resize((512,512))\n",
    "        real = Image.open(request.files[\"real\"]).convert(\"RGB\").resize((512,512))\n",
    "\n",
    "        mask_gen = compute_water_mask_rgb(gen)\n",
    "        mask_real = compute_water_mask_rgb(real)\n",
    "\n",
    "        TP = int(np.sum((mask_gen>0)&(mask_real>0)))\n",
    "        FP = int(np.sum((mask_gen>0)&(mask_real==0)))\n",
    "        FN = int(np.sum((mask_gen==0)&(mask_real>0)))\n",
    "        TN = int(np.sum((mask_gen==0)&(mask_real==0)))\n",
    "\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN+1e-9)*100\n",
    "        precision = TP/(TP+FP+1e-9)\n",
    "        recall = TP/(TP+FN+1e-9)\n",
    "        f1 = 2*precision*recall/(precision+recall+1e-9)\n",
    "\n",
    "        ssim_v = ssim(\n",
    "            np.array(real.convert(\"L\")),\n",
    "            np.array(gen.convert(\"L\"))\n",
    "        )\n",
    "        fid_v = compute_fid(real, gen)\n",
    "\n",
    "        return jsonify({\n",
    "            \"accuracy\": round(accuracy,3),\n",
    "            \"precision\": round(precision,3),\n",
    "            \"recall\": round(recall,3),\n",
    "            \"f1\": round(f1,3),\n",
    "            \"ssim\": round(ssim_v,3),\n",
    "            \"fid\": round(fid_v,3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# ============================================================\n",
    "# 12) START SERVER\n",
    "# ============================================================\n",
    "\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(\"ðŸš€ BACKEND URL:\", public_url)\n",
    "\n",
    "app.run(host=\"0.0.0.0\", port=5000, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
